{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import FastText\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('//Users/urmi/Documents/NLP/Assignment_1/pmc-data-all.csv',usecols=['Article_Id', 'Article_Title', 'Abstract'])\n",
    "df.shape\n",
    "#, nrows=30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.loc[len(df.index)] = [0,'justjustjust','A 21-year-old college student undergoes colonoscopy due to family history of multiple polyps in his older siblings. His brother underwent total proctocolectomy at age 22, and his sister underwent a total proctocolectomy at age 28, after both were found to have hundreds of colonic adenomas on colonoscopy. Both siblings are currently well without any findings of neoplasms. The patient undergoes sigmoidoscopy and is found to have dozens of small colonic polyps within rectosigmoid. Several of these are biopsied and are all benign adenomas.'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.loc[205935].Article_Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = df['Article_Id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2bebded00eb88856ae289d8e27baa81e7a6801f2"
   },
   "outputs": [],
   "source": [
    "#Transforming questions to list for ease of processing\n",
    "abstract_list = df['Abstract'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "40d5e8709a8fa8b1d168c12822752a2d8f165828"
   },
   "outputs": [],
   "source": [
    "#Tokenizing with simple preprocess gensim's simple preprocess\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(simple_preprocess(str(sentence), deacc=True)) # returns lowercase tokens, ignoring tokens that are too short or too long\n",
    "\n",
    "abstract_words = list(sent_to_words(abstract_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3ea7fc61033cf88bc11480d35955e35565935aee"
   },
   "outputs": [],
   "source": [
    "#Getting stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(sentence):\n",
    "  filtered_words = [word for word in sentence if word not in stop_words]\n",
    "  return filtered_words\n",
    "\n",
    "abstract_questions = [remove_stopwords(question) for question in abstract_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#print(filtered_questions[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from nltk.stem.porter import *\n",
    "#stemmer = PorterStemmer()\n",
    "#stem_questions = [stemmer.stem(item) for item in filtered_questions]\n",
    "#stem_questions = [[stemmer.stem(word) for word in item] for item in filtered_questions]\n",
    "#singles = [stemmer.stem(plural) for plural in plurals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "59397ff63e9a1fc72823c74e1bb2b2699521c6bb"
   },
   "outputs": [],
   "source": [
    "#len(filtered_questions)\n",
    "#len(stem_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e2283facaf0ef42a0d13d625415f0ff264e9bfbf"
   },
   "outputs": [],
   "source": [
    "#Instantiating the word2vec model\n",
    "n = 300\n",
    "model = Word2Vec(abstract_questions, size = n, window = 8)\n",
    "\n",
    "#Training model \n",
    "model.train(abstract_questions, total_examples=len(abstract_questions), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ecfe822d97fc72b0d457516ab2a05c4c132fb63a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_vectors = model.wv\n",
    "print('Words similar to \"health\" are: ', word_vectors.most_similar(positive='health'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7980a123b38e246191dc18ea827d2ed3d7f1d0e7"
   },
   "outputs": [],
   "source": [
    "#fasttext model\n",
    "ft_model = FastText(abstract_questions, size=n, window=8, min_count=5, workers=2,sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c0333451b4c1514cb1e47a3b61b2f6f812e815a2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Words similar to \"health\" are: ', ft_model.wv.most_similar('health'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6ad852e394420e1e98b13d9c76085e8d2e3b1a3f"
   },
   "outputs": [],
   "source": [
    "# code for tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(abstract_list)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7a4c41a3fa84f3b024dd4b9d4719ae7899ffa972"
   },
   "outputs": [],
   "source": [
    "#To proprely work with scikit's vectorizer\n",
    "merged_questions = [' '.join(question) for question in abstract_questions]\n",
    "document_names = ['Doc {:d}'.format(i) for i in range(len(merged_questions))]\n",
    "\n",
    "def get_tfidf(docs, ngram_range=(1,1), index=None):\n",
    "    vect = TfidfVectorizer(stop_words='english', ngram_range=ngram_range)\n",
    "    tfidf = vect.fit_transform(docs).todense()\n",
    "    return pd.DataFrame(tfidf, columns=vect.get_feature_names(), index=index).T\n",
    "\n",
    "tfidf = get_tfidf(merged_questions, ngram_range=(1,1), index=document_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fd01992d62b205fa749cefe55552a153792c7cfc"
   },
   "outputs": [],
   "source": [
    "#centroid function\n",
    "def get_sent_embs(emb_model):\n",
    "    sent_embs = []\n",
    "    for desc in range(len(abstract_questions)):\n",
    "        sent_emb = np.zeros((1, n))\n",
    "        if len(stem_questions[desc]) > 0:\n",
    "            sent_emb = np.zeros((1, n))\n",
    "            div = 0\n",
    "            model = emb_model\n",
    "            for word in abstract_questions[desc]:\n",
    "                if word in model.wv.vocab and word in tfidf.index:\n",
    "                    word_emb = model.wv[word]\n",
    "                    weight = tfidf.loc[word, 'Doc {:d}'.format(desc)]\n",
    "                    sent_emb = np.add(sent_emb, word_emb * weight)\n",
    "                    div += weight\n",
    "                else:\n",
    "                    div += 1e-13 #to avoid dividing by 0\n",
    "        if div == 0:\n",
    "            print(desc)\n",
    "\n",
    "        sent_emb = np.divide(sent_emb, div)\n",
    "        sent_embs.append(sent_emb.flatten())\n",
    "    return sent_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8f2526b714ebd3e7d57bf1fd3801b166a5c5184a"
   },
   "outputs": [],
   "source": [
    "ft_sent = get_sent_embs(emb_model = ft_model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "66696ffbc347158a145eaa9c3c80fe3e1352b7f6"
   },
   "outputs": [],
   "source": [
    "#to find similarity\n",
    "def get_n_most_similar(interest_index, embeddings, n):\n",
    "    \n",
    "    nbrs = NearestNeighbors(n_neighbors=n, metric='cosine').fit(embeddings)\n",
    "    distances, indices = nbrs.kneighbors(embeddings)\n",
    "    #print(distances)\n",
    "    #print(indices)\n",
    "    similar_indices = indices[interest_index][1:]\n",
    "    similar_distances = distances[interest_index][1:]\n",
    "    #print (similar_distances)\n",
    "    #print(similar_indices)\n",
    "    return similar_indices, similar_distances\n",
    "\n",
    "def print_similar(interest_index, embeddings, n):\n",
    "    \n",
    "    closest_ind, closest_dist = get_n_most_similar(interest_index, embeddings, n)\n",
    "    print('Question : \\t %s \\n \\n is most similar to these %s questions: \\n' % (abstract_list[interest_index], n))\n",
    "    closest_ind = closest_ind[::-1]\n",
    "    closest_dist = closest_dist[::-1]\n",
    "    \n",
    "    with open('test_output.txt','a') as file:\n",
    "        file.write('TOPIC_NO Q0 PMCID RANK SCORE RUN_NAME\\n')\n",
    "        for idx,question in enumerate(closest_ind):\n",
    "            file.write(f'{30} {0} {id_list[question]} {idx} {closest_dist[idx]} {\"file_30\"}\\n')\n",
    "            print('Article_Id :', id_list[question],',','Score:' ,closest_dist[idx],': ',abstract_list[question],'\\n')\n",
    "            #file.write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2d7afc4ef1fd1b359f8356479a330bee0b70e7cc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test = ['58-year-old', 'african-american', 'woman', 'present', 'ER', 'episod', 'pressing/burn', 'anterior', 'chest', 'pain', 'began', 'two', 'day', 'earlier', 'first', 'time', 'life', 'pain', 'start', 'walk', 'radiat', 'back', 'accompani', 'nausea', 'diaphoresi', 'mild', 'dyspnea', 'increas', 'inspir', 'latest', 'episod', 'pain', 'end', 'half', 'hour', 'prior', 'arriv', 'known', 'hypertens', 'obes', 'deni', 'smoke', 'diabet', 'hypercholesterolemia', 'famili', 'histori', 'heart', 'diseas', 'current', 'take', 'medic', 'physic', 'examin', 'normal', 'ekg', 'show', 'nonspecif', 'chang']\n",
    "print_similar(216864, ft_sent, 1000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
